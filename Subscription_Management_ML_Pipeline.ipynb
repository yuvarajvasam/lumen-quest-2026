
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subscription Management System - Complete ML Pipeline\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements a comprehensive machine learning pipeline for a subscription management system that includes:\n",
    "\n",
    "1. **Churn Prediction Model** - Identify customers at risk of canceling\n",
    "2. **Plan Recommendation Engine** - Suggest optimal subscription plans\n",
    "3. **Usage Pattern Analysis** - Analyze customer data consumption behavior\n",
    "4. **Pricing Strategy Model** - Optimize pricing for customer segments\n",
    "5. **Customer Lifetime Value (CLV) Prediction** - Forecast customer value\n",
    "\n",
    "### Dataset Description\n",
    "Our subscription management dataset contains 10,000 customers with features including:\n",
    "- **Demographics**: Age, gender, location, income bracket, family size\n",
    "- **Subscription Details**: Plan type, contract duration, monthly charges, data quotas\n",
    "- **Usage Patterns**: Data consumption, peak usage times, support interactions\n",
    "- **Behavioral Indicators**: Payment history, auto-renewal settings, tenure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print('✅ Libraries imported successfully!')\n",
    "print('📊 Ready to build subscription management ML pipeline!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Generation\n",
    "\n",
    "We'll create a comprehensive synthetic dataset that mirrors real-world subscription management scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subscription_dataset(n_customers=10000):\n",
    "    \"\"\"\n",
    "    Generate a realistic subscription management dataset\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Customer demographics\n",
    "    customer_data = {\n",
    "        'customer_id': range(1, n_customers + 1),\n",
    "        'age': np.random.normal(35, 12, n_customers).astype(int),\n",
    "        'gender': np.random.choice(['Male', 'Female'], n_customers),\n",
    "        'location': np.random.choice(['Urban', 'Suburban', 'Rural'], n_customers, p=[0.5, 0.3, 0.2]),\n",
    "        'income_bracket': np.random.choice(['Low', 'Medium', 'High'], n_customers, p=[0.3, 0.5, 0.2]),\n",
    "        'family_size': np.random.poisson(2.5, n_customers) + 1,\n",
    "    }\n",
    "    \n",
    "    # Subscription details\n",
    "    subscription_types = ['Fibernet_Basic', 'Fibernet_Premium', 'Broadband_Copper_Basic', 'Broadband_Copper_Premium']\n",
    "    contract_types = ['Monthly', 'Quarterly', 'Yearly']\n",
    "    \n",
    "    subscription_data = {\n",
    "        'subscription_type': np.random.choice(subscription_types, n_customers),\n",
    "        'contract_type': np.random.choice(contract_types, n_customers, p=[0.6, 0.25, 0.15]),\n",
    "        'monthly_charge': np.random.uniform(25, 150, n_customers),\n",
    "        'data_quota_gb': np.random.choice([50, 100, 200, 500, 1000], n_customers),\n",
    "        'tenure_months': np.random.exponential(18, n_customers).astype(int),\n",
    "    }\n",
    "    \n",
    "    # Usage patterns\n",
    "    usage_data = {\n",
    "        'avg_monthly_usage_gb': np.random.lognormal(4, 1, n_customers),\n",
    "        'peak_usage_hours': np.random.choice(['Morning', 'Afternoon', 'Evening', 'Night'], n_customers, p=[0.15, 0.25, 0.45, 0.15]),\n",
    "        'support_tickets_3m': np.random.poisson(1.5, n_customers),\n",
    "        'payment_delays_6m': np.random.poisson(0.5, n_customers),\n",
    "        'auto_renew': np.random.choice([0, 1], n_customers, p=[0.3, 0.7]),\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({**customer_data, **subscription_data, **usage_data})\n",
    "    \n",
    "    # Create realistic churn based on multiple factors\n",
    "    churn_probability = (\n",
    "        0.1 +  # base churn rate\n",
    "        0.3 * (df['monthly_charge'] > 100).astype(int) +  # high price increases churn\n",
    "        0.2 * (df['support_tickets_3m'] > 3).astype(int) +  # many support issues\n",
    "        0.15 * (df['payment_delays_6m'] > 1).astype(int) +  # payment issues\n",
    "        0.1 * (df['contract_type'] == 'Monthly').astype(int) +  # monthly contracts less sticky\n",
    "        0.1 * (df['avg_monthly_usage_gb'] > df['data_quota_gb']).astype(int) -  # quota exceeded\n",
    "        0.15 * (df['auto_renew'] == 1).astype(int) -  # auto renew reduces churn\n",
    "        0.1 * (df['tenure_months'] > 24).astype(int)  # loyalty reduces churn\n",
    "    )\n",
    "    \n",
    "    df['churn'] = np.random.binomial(1, np.clip(churn_probability, 0, 1), n_customers)\n",
    "    \n",
    "    # Add derived features\n",
    "    df['usage_quota_ratio'] = df['avg_monthly_usage_gb'] / df['data_quota_gb']\n",
    "    df['price_per_gb'] = df['monthly_charge'] / df['data_quota_gb']\n",
    "    df['is_heavy_user'] = (df['usage_quota_ratio'] > 0.8).astype(int)\n",
    "    df['is_premium_customer'] = (df['monthly_charge'] > 80).astype(int)\n",
    "    \n",
    "    # Calculate Customer Lifetime Value (CLV)\n",
    "    df['clv'] = (df['monthly_charge'] * df['tenure_months'] * (1 - df['churn'] * 0.5)).round(2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate the dataset\n",
    "df = generate_subscription_dataset(10000)\n",
    "\n",
    "print('🎯 Dataset Generated Successfully!')\n",
    "print(f'📊 Total customers: {len(df):,}')\n",
    "print(f'📈 Features: {df.shape[1]}')\n",
    "print(f'❌ Churn rate: {df["churn"].mean():.2%}')\n",
    "print(f'💰 Average CLV: ${df["clv"].mean():,.2f}')\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset overview\n",
    "print('📋 DATASET OVERVIEW')\n",
    "print('=' * 50)\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'Missing values: {df.isnull().sum().sum()}')\n",
    "print('\n📊 NUMERICAL FEATURES SUMMARY')\n",
    "print('=' * 50)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features analysis\n",
    "categorical_features = ['gender', 'location', 'income_bracket', 'subscription_type', 'contract_type', 'peak_usage_hours']\n",
    "\n",
    "print('📊 CATEGORICAL FEATURES DISTRIBUTION')\n",
    "print('=' * 50)\n",
    "for feature in categorical_features:\n",
    "    print(f'\n{feature.upper()}:')\n",
    "    print(df[feature].value_counts())\n",
    "    print(f'Churn rate by {feature}:')\n",
    "    churn_by_feature = df.groupby(feature)['churn'].agg(['count', 'mean']).round(3)\n",
    "    churn_by_feature.columns = ['Count', 'Churn_Rate']\n",
    "    print(churn_by_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Select numerical columns for correlation\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "# Create heatmap\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={'shrink': 0.5}, fmt='.2f')\n",
    "\n",
    "plt.title('🔗 Feature Correlation Heatmap', fontsize=16, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show strongest correlations with churn\n",
    "print('🎯 STRONGEST CORRELATIONS WITH CHURN:')\n",
    "print('=' * 50)\n",
    "churn_corr = correlation_matrix['churn'].drop('churn').sort_values(key=abs, ascending=False)\n",
    "for feature, corr in churn_corr.head(10).items():\n",
    "    print(f'{feature}: {corr:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, target_col='churn'):\n",
    "    \"\"\"\n",
    "    Comprehensive data preprocessing pipeline\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    categorical_features = ['gender', 'location', 'income_bracket', 'subscription_type', 'contract_type', 'peak_usage_hours']\n",
    "    \n",
    "    # Label encode binary categorical variables\n",
    "    le_gender = LabelEncoder()\n",
    "    df_processed['gender_encoded'] = le_gender.fit_transform(df_processed['gender'])\n",
    "    \n",
    "    # One-hot encode multi-category variables\n",
    "    df_encoded = pd.get_dummies(df_processed, columns=['location', 'income_bracket', 'subscription_type', 'contract_type', 'peak_usage_hours'], \n",
    "                               prefix=['loc', 'income', 'sub', 'contract', 'peak'])\n",
    "    \n",
    "    # Drop original categorical columns\n",
    "    df_encoded = df_encoded.drop(['gender'], axis=1)\n",
    "    \n",
    "    # Feature engineering\n",
    "    df_encoded['price_efficiency'] = df_encoded['data_quota_gb'] / df_encoded['monthly_charge']\n",
    "    df_encoded['support_intensity'] = df_encoded['support_tickets_3m'] / (df_encoded['tenure_months'] + 1)\n",
    "    df_encoded['loyalty_score'] = df_encoded['tenure_months'] * (1 - df_encoded['payment_delays_6m'] * 0.1)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "# Preprocess the data\n",
    "df_processed = preprocess_data(df)\n",
    "\n",
    "print('🔄 Data Preprocessing Completed!')\n",
    "print(f'📊 Original features: {df.shape[1]}')\n",
    "print(f'📈 Processed features: {df_processed.shape[1]}')\n",
    "print(f'✅ No missing values: {df_processed.isnull().sum().sum() == 0}')\n",
    "\n",
    "# Show new features\n",
    "print('\n🆕 New engineered features:')\n",
    "new_features = ['price_efficiency', 'support_intensity', 'loyalty_score']\n",
    "print(df_processed[new_features].describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model 1: Churn Prediction\n",
    "\n",
    "Predicting which customers are likely to cancel their subscriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_churn_prediction_model(df_processed):\n",
    "    \"\"\"\n",
    "    Build and evaluate churn prediction models\n",
    "    \"\"\"\n",
    "    # Prepare features and target\n",
    "    X = df_processed.drop(['customer_id', 'churn'], axis=1)\n",
    "    y = df_processed['churn']\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print('🤖 CHURN PREDICTION MODEL TRAINING')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f'\nTraining {name}...')\n",
    "        \n",
    "        # Use scaled data for logistic regression, original for tree-based models\n",
    "        if name == 'Logistic Regression':\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        results[name] = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1,\n",
    "            'AUC-ROC': auc,\n",
    "            'Model': model\n",
    "        }\n",
    "        \n",
    "        print(f'✅ {name} completed!')\n",
    "        print(f'   Accuracy: {accuracy:.3f} | Precision: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f} | AUC: {auc:.3f}')\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({k: {metric: v[metric] for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']} \n",
    "                              for k, v in results.items()}).T\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_name = results_df['AUC-ROC'].idxmax()\n",
    "    best_model = results[best_model_name]['Model']\n",
    "    \n",
    "    print('\n🏆 MODEL PERFORMANCE COMPARISON')\n",
    "    print('=' * 60)\n",
    "    print(results_df.round(3))\n",
    "    print(f'\n🥇 Best model: {best_model_name} (AUC-ROC: {results_df.loc[best_model_name, "AUC-ROC"]:.3f})')\n",
    "    \n",
    "    return best_model, results_df, X.columns, scaler\n",
    "\n",
    "# Train churn prediction models\n",
    "churn_model, churn_results, feature_names, churn_scaler = build_churn_prediction_model(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis for best churn model\n",
    "if hasattr(churn_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': churn_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print('🎯 TOP 15 MOST IMPORTANT FEATURES FOR CHURN PREDICTION')\n",
    "    print('=' * 60)\n",
    "    for i, (_, row) in enumerate(feature_importance.head(15).iterrows(), 1):\n",
    "        print(f'{i:2d}. {row["feature"]:25s}: {row["importance"]:.4f}')\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'], color='skyblue')\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('🎯 Top 15 Features for Churn Prediction', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model 2: Plan Recommendation System\n",
    "\n",
    "Recommending the most suitable subscription plans for customers based on their usage patterns and preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_plan_recommendation_system(df):\n",
    "    \"\"\"\n",
    "    Build a plan recommendation system using collaborative filtering and content-based approaches\n",
    "    \"\"\"\n",
    "    print('🎯 BUILDING PLAN RECOMMENDATION SYSTEM')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Create customer profiles for recommendation\n",
    "    customer_features = ['age', 'family_size', 'avg_monthly_usage_gb', 'income_bracket', \n",
    "                        'location', 'is_heavy_user', 'support_tickets_3m']\n",
    "    \n",
    "    # Encode categorical features for recommendation\n",
    "    df_rec = df.copy()\n",
    "    le_income = LabelEncoder()\n",
    "    le_location = LabelEncoder()\n",
    "    \n",
    "    df_rec['income_encoded'] = le_income.fit_transform(df_rec['income_bracket'])\n",
    "    df_rec['location_encoded'] = le_location.fit_transform(df_rec['location'])\n",
    "    \n",
    "    # Customer feature matrix\n",
    "    feature_cols = ['age', 'family_size', 'avg_monthly_usage_gb', 'income_encoded', \n",
    "                   'location_encoded', 'is_heavy_user', 'support_tickets_3m']\n",
    "    \n",
    "    X_customers = df_rec[feature_cols].values\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler_rec = StandardScaler()\n",
    "    X_customers_scaled = scaler_rec.fit_transform(X_customers)\n",
    "    \n",
    "    # Use KMeans to create customer segments\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "    customer_segments = kmeans.fit_predict(X_customers_scaled)\n",
    "    df_rec['customer_segment'] = customer_segments\n",
    "    \n",
    "    # Analyze segments and recommend plans\n",
    "    segment_analysis = df_rec.groupby('customer_segment').agg({\n",
    "        'age': 'mean',\n",
    "        'family_size': 'mean',\n",
    "        'avg_monthly_usage_gb': 'mean',\n",
    "        'monthly_charge': 'mean',\n",
    "        'data_quota_gb': 'mean',\n",
    "        'subscription_type': lambda x: x.mode()[0],\n",
    "        'churn': 'mean',\n",
    "        'customer_id': 'count'\n",
    "    }).round(2)\n",
    "    \n",
    "    segment_analysis.columns = ['Avg_Age', 'Avg_Family_Size', 'Avg_Usage_GB', 'Avg_Monthly_Charge', \n",
    "                               'Avg_Quota_GB', 'Most_Popular_Plan', 'Churn_Rate', 'Count']\n",
    "    \n",
    "    print('🔍 CUSTOMER SEGMENTS ANALYSIS')\n",
    "    print('=' * 60)\n",
    "    print(segment_analysis)\n",
    "    \n",
    "    # Plan recommendation logic\n",
    "    def recommend_plan(usage_gb, family_size, income_encoded, age):\n",
    "        \"\"\"\n",
    "        Recommend optimal plan based on customer characteristics\n",
    "        \"\"\"\n",
    "        # Heavy user with high income\n",
    "        if usage_gb > 200 and income_encoded == 2:  # High income\n",
    "            return 'Fibernet_Premium'\n",
    "        # Heavy user with medium income\n",
    "        elif usage_gb > 200 and income_encoded == 1:  # Medium income\n",
    "            return 'Fibernet_Basic'\n",
    "        # Large family\n",
    "        elif family_size > 4:\n",
    "            return 'Fibernet_Premium'\n",
    "        # Light user with low income\n",
    "        elif usage_gb < 50 and income_encoded == 0:  # Low income\n",
    "            return 'Broadband_Copper_Basic'\n",
    "        # Medium usage\n",
    "        elif 50 <= usage_gb <= 200:\n",
    "            return 'Broadband_Copper_Premium'\n",
    "        else:\n",
    "            return 'Fibernet_Basic'\n",
    "    \n",
    "    # Generate recommendations\n",
    "    df_rec['recommended_plan'] = df_rec.apply(\n",
    "        lambda row: recommend_plan(row['avg_monthly_usage_gb'], row['family_size'], \n",
    "                                  row['income_encoded'], row['age']), axis=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate recommendation accuracy\n",
    "    recommendation_accuracy = (df_rec['subscription_type'] == df_rec['recommended_plan']).mean()\n",
    "    \n",
    "    print(f'\n🎯 Recommendation Accuracy: {recommendation_accuracy:.3f}')\n",
    "    \n",
    "    # Analyze recommendations by segment\n",
    "    rec_by_segment = df_rec.groupby('customer_segment')['recommended_plan'].value_counts().unstack(fill_value=0)\n",
    "    print('\n💡 RECOMMENDED PLANS BY CUSTOMER SEGMENT')\n",
    "    print('=' * 60)\n",
    "    print(rec_by_segment)\n",
    "    \n",
    "    return df_rec, kmeans, scaler_rec, le_income, le_location, segment_analysis\n",
    "\n",
    "# Build recommendation system\n",
    "df_with_recs, rec_kmeans, rec_scaler, rec_le_income, rec_le_location, segments_info = build_plan_recommendation_system(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model 3: Usage Pattern Analysis\n",
    "\n",
    "Analyzing customer data consumption patterns to optimize network resources and identify trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_usage_patterns(df):\n",
    "    \"\"\"\n",
    "    Comprehensive usage pattern analysis\n",
    "    \"\"\"\n",
    "    print('📊 USAGE PATTERN ANALYSIS')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Usage statistics by different dimensions\n",
    "    print('\n1. USAGE STATISTICS BY SUBSCRIPTION TYPE')\n",
    "    usage_by_plan = df.groupby('subscription_type').agg({\n",
    "        'avg_monthly_usage_gb': ['mean', 'median', 'std'],\n",
    "        'usage_quota_ratio': ['mean', 'median'],\n",
    "        'customer_id': 'count'\n",
    "    }).round(2)\n",
    "    print(usage_by_plan)\n",
    "    \n",
    "    print('\n2. USAGE STATISTICS BY LOCATION')\n",
    "    usage_by_location = df.groupby('location').agg({\n",
    "        'avg_monthly_usage_gb': ['mean', 'median'],\n",
    "        'monthly_charge': 'mean',\n",
    "        'support_tickets_3m': 'mean'\n",
    "    }).round(2)\n",
    "    print(usage_by_location)\n",
    "    \n",
    "    print('\n3. PEAK USAGE TIME ANALYSIS')\n",
    "    peak_time_analysis = df.groupby('peak_usage_hours').agg({\n",
    "        'avg_monthly_usage_gb': 'mean',\n",
    "        'churn': 'mean',\n",
    "        'customer_id': 'count'\n",
    "    }).round(2)\n",
    "    peak_time_analysis.columns = ['Avg_Usage_GB', 'Churn_Rate', 'Customer_Count']\n",
    "    print(peak_time_analysis)\n",
    "    \n",
    "    # Heavy users analysis\n",
    "    heavy_users = df[df['is_heavy_user'] == 1]\n",
    "    print(f'\n4. HEAVY USERS ANALYSIS (Usage > 80% of quota)')\n",
    "    print(f'Heavy users count: {len(heavy_users):,} ({len(heavy_users)/len(df):.1%})')\n",
    "    print(f'Average usage: {heavy_users["avg_monthly_usage_gb"].mean():.1f} GB')\n",
    "    print(f'Average quota: {heavy_users["data_quota_gb"].mean():.1f} GB')\n",
    "    print(f'Churn rate among heavy users: {heavy_users["churn"].mean():.2%}')\n",
    "    print(f'Average monthly charge: ${heavy_users["monthly_charge"].mean():.2f}')\n",
    "    \n",
    "    # Usage efficiency analysis\n",
    "    print('\n5. QUOTA UTILIZATION ANALYSIS')\n",
    "    df['usage_efficiency'] = pd.cut(df['usage_quota_ratio'], \n",
    "                                   bins=[0, 0.3, 0.7, 1.0, float('inf')],\n",
    "                                   labels=['Under-utilized', 'Moderate', 'High', 'Over-quota'])\n",
    "    \n",
    "    efficiency_analysis = df.groupby('usage_efficiency').agg({\n",
    "        'customer_id': 'count',\n",
    "        'churn': 'mean',\n",
    "        'monthly_charge': 'mean',\n",
    "        'support_tickets_3m': 'mean'\n",
    "    }).round(2)\n",
    "    efficiency_analysis.columns = ['Count', 'Churn_Rate', 'Avg_Charge', 'Avg_Support_Tickets']\n",
    "    print(efficiency_analysis)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Usage distribution\n",
    "    axes[0,0].hist(df['avg_monthly_usage_gb'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0,0].set_title('📊 Distribution of Monthly Usage')\n",
    "    axes[0,0].set_xlabel('Average Monthly Usage (GB)')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Usage by subscription type\n",
    "    usage_summary = df.groupby('subscription_type')['avg_monthly_usage_gb'].mean().sort_values(ascending=True)\n",
    "    axes[0,1].barh(usage_summary.index, usage_summary.values, color='lightcoral')\n",
    "    axes[0,1].set_title('📊 Average Usage by Subscription Type')\n",
    "    axes[0,1].set_xlabel('Average Monthly Usage (GB)')\n",
    "    \n",
    "    # Peak usage times\n",
    "    peak_counts = df['peak_usage_hours'].value_counts()\n",
    "    axes[1,0].pie(peak_counts.values, labels=peak_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[1,0].set_title('⏰ Peak Usage Time Distribution')\n",
    "    \n",
    "    # Usage vs Quota scatter\n",
    "    scatter_colors = ['red' if churn else 'blue' for churn in df['churn']]\n",
    "    axes[1,1].scatter(df['data_quota_gb'], df['avg_monthly_usage_gb'], \n",
    "                     alpha=0.5, c=scatter_colors, s=20)\n",
    "    axes[1,1].plot([0, df['data_quota_gb'].max()], [0, df['data_quota_gb'].max()], \n",
    "                  'k--', alpha=0.8, label='Quota line')\n",
    "    axes[1,1].set_title('📈 Usage vs Quota (Red=Churned, Blue=Active)')\n",
    "    axes[1,1].set_xlabel('Data Quota (GB)')\n",
    "    axes[1,1].set_ylabel('Average Monthly Usage (GB)')\n",
    "    axes[1,1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return efficiency_analysis, peak_time_analysis\n",
    "\n",
    "# Perform usage analysis\n",
    "efficiency_stats, peak_stats = analyze_usage_patterns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model 4: Pricing Strategy Optimization\n",
    "\n",
    "Optimizing pricing strategies based on customer segments and market analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_pricing_strategy(df):\n",
    "    \"\"\"\n",
    "    Develop pricing optimization recommendations\n",
    "    \"\"\"\n",
    "    print('💰 PRICING STRATEGY OPTIMIZATION')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Price sensitivity analysis\n",
    "    print('\n1. PRICE SENSITIVITY ANALYSIS')\n",
    "    \n",
    "    # Create price bins\n",
    "    df['price_tier'] = pd.cut(df['monthly_charge'], \n",
    "                             bins=[0, 50, 80, 120, float('inf')],\n",
    "                             labels=['Budget', 'Standard', 'Premium', 'Enterprise'])\n",
    "    \n",
    "    price_analysis = df.groupby('price_tier').agg({\n",
    "        'customer_id': 'count',\n",
    "        'churn': 'mean',\n",
    "        'avg_monthly_usage_gb': 'mean',\n",
    "        'clv': 'mean',\n",
    "        'monthly_charge': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    price_analysis.columns = ['Customer_Count', 'Churn_Rate', 'Avg_Usage_GB', 'Avg_CLV', 'Avg_Price']\n",
    "    print(price_analysis)\n",
    "    \n",
    "    # Revenue optimization by segment\n",
    "    print('\n2. REVENUE ANALYSIS BY CUSTOMER SEGMENT')\n",
    "    \n",
    "    # Calculate revenue metrics\n",
    "    df['monthly_revenue'] = df['monthly_charge']\n",
    "    df['annual_revenue_potential'] = df['monthly_charge'] * 12 * (1 - df['churn'])\n",
    "    \n",
    "    revenue_by_segment = df.groupby('location').agg({\n",
    "        'monthly_revenue': 'sum',\n",
    "        'annual_revenue_potential': 'sum',\n",
    "        'churn': 'mean',\n",
    "        'customer_id': 'count'\n",
    "    }).round(2)\n",
    "    \n",
    "    revenue_by_segment.columns = ['Monthly_Revenue', 'Annual_Revenue_Potential', 'Churn_Rate', 'Customer_Count']\n",
    "    print(revenue_by_segment)\n",
    "    \n",
    "    # Price elasticity analysis\n",
    "    print('\n3. OPTIMAL PRICING RECOMMENDATIONS')\n",
    "    \n",
    "    # Calculate price per GB for each plan\n",
    "    plan_pricing = df.groupby('subscription_type').agg({\n",
    "        'monthly_charge': 'mean',\n",
    "        'data_quota_gb': 'mean',\n",
    "        'price_per_gb': 'mean',\n",
    "        'churn': 'mean',\n",
    "        'avg_monthly_usage_gb': 'mean',\n",
    "        'customer_id': 'count'\n",
    "    }).round(2)\n",
    "    \n",
    "    plan_pricing['value_score'] = (plan_pricing['data_quota_gb'] / plan_pricing['monthly_charge'] * 100).round(2)\n",
    "    plan_pricing.columns = ['Avg_Price', 'Avg_Quota_GB', 'Price_per_GB', 'Churn_Rate', \n",
    "                           'Avg_Usage_GB', 'Customer_Count', 'Value_Score']\n",
    "    \n",
    "    print(plan_pricing.sort_values('Value_Score', ascending=False))\n",
    "    \n",
    "    # Pricing recommendations\n",
    "    print('\n4. STRATEGIC PRICING RECOMMENDATIONS')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    for plan_type in plan_pricing.index:\n",
    "        churn_rate = plan_pricing.loc[plan_type, 'Churn_Rate']\n",
    "        price_per_gb = plan_pricing.loc[plan_type, 'Price_per_GB']\n",
    "        customer_count = plan_pricing.loc[plan_type, 'Customer_Count']\n",
    "        \n",
    "        if churn_rate > 0.25:  # High churn\n",
    "            recommendations.append(f'{plan_type}: 📉 REDUCE price by 10-15% (High churn: {churn_rate:.2%})')\n",
    "        elif churn_rate < 0.15 and customer_count > 1000:  # Low churn, popular plan\n",
    "            recommendations.append(f'{plan_type}: 📈 INCREASE price by 5-10% (Low churn: {churn_rate:.2%}, Popular plan)')\n",
    "        elif price_per_gb > 1.5:  # Expensive per GB\n",
    "            recommendations.append(f'{plan_type}: 💡 OPTIMIZE quota (High price per GB: ${price_per_gb:.2f})')\n",
    "        else:\n",
    "            recommendations.append(f'{plan_type}: ✅ MAINTAIN current pricing (Balanced metrics)')\n",
    "    \n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f'{i}. {rec}')\n",
    "    \n",
    "    # Revenue impact simulation\n",
    "    print('\n5. REVENUE IMPACT SIMULATION')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    current_monthly_revenue = df['monthly_charge'].sum()\n",
    "    \n",
    "    # Simulate 5% price increase on low-churn plans\n",
    "    low_churn_plans = plan_pricing[plan_pricing['Churn_Rate'] < 0.15].index\n",
    "    price_increase_impact = 0\n",
    "    \n",
    "    for plan in low_churn_plans:\n",
    "        plan_customers = df[df['subscription_type'] == plan]['customer_id'].count()\n",
    "        avg_price = plan_pricing.loc[plan, 'Avg_Price']\n",
    "        price_increase_impact += plan_customers * avg_price * 0.05  # 5% increase\n",
    "    \n",
    "    print(f'Current monthly revenue: ${current_monthly_revenue:,.2f}')\n",
    "    print(f'Potential additional revenue (5% increase on low-churn plans): ${price_increase_impact:,.2f}')\n",
    "    print(f'Total potential monthly revenue: ${current_monthly_revenue + price_increase_impact:,.2f}')\n",
    "    print(f'Annual revenue impact: ${(price_increase_impact * 12):,.2f}')\n",
    "    \n",
    "    return plan_pricing, revenue_by_segment, recommendations\n",
    "\n",
    "# Optimize pricing strategy\n",
    "pricing_analysis, revenue_analysis, pricing_recommendations = optimize_pricing_strategy(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model 5: Customer Lifetime Value (CLV) Prediction\n",
    "\n",
    "Predicting the total value a customer will bring over their entire relationship with the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clv_prediction_model(df_processed):\n",
    "    \"\"\"\n",
    "    Build Customer Lifetime Value prediction model\n",
    "    \"\"\"\n",
    "    print('💎 CUSTOMER LIFETIME VALUE PREDICTION')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Prepare features for CLV prediction\n",
    "    clv_features = ['age', 'family_size', 'monthly_charge', 'data_quota_gb', 'avg_monthly_usage_gb',\n",
    "                   'tenure_months', 'support_tickets_3m', 'payment_delays_6m', 'auto_renew',\n",
    "                   'usage_quota_ratio', 'price_per_gb', 'is_heavy_user', 'is_premium_customer']\n",
    "    \n",
    "    X_clv = df_processed[clv_features]\n",
    "    y_clv = df_processed['clv']\n",
    "    \n",
    "    # Split data\n",
    "    X_train_clv, X_test_clv, y_train_clv, y_test_clv = train_test_split(X_clv, y_clv, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler_clv = StandardScaler()\n",
    "    X_train_clv_scaled = scaler_clv.fit_transform(X_train_clv)\n",
    "    X_test_clv_scaled = scaler_clv.transform(X_test_clv)\n",
    "    \n",
    "    # Train multiple regression models\n",
    "    clv_models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "    }\n",
    "    \n",
    "    clv_results = {}\n",
    "    \n",
    "    for name, model in clv_models.items():\n",
    "        print(f'\nTraining {name} for CLV prediction...')\n",
    "        \n",
    "        if name == 'Linear Regression':\n",
    "            model.fit(X_train_clv_scaled, y_train_clv)\n",
    "            y_pred_clv = model.predict(X_test_clv_scaled)\n",
    "        else:\n",
    "            model.fit(X_train_clv, y_train_clv)\n",
    "            y_pred_clv = model.predict(X_test_clv)\n",
    "        \n",
    "        # Calculate regression metrics\n",
    "        mse = mean_squared_error(y_test_clv, y_pred_clv)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test_clv, y_pred_clv)\n",
    "        r2 = r2_score(y_test_clv, y_pred_clv)\n",
    "        \n",
    "        clv_results[name] = {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R2_Score': r2,\n",
    "            'Model': model\n",
    "        }\n",
    "        \n",
    "        print(f'✅ {name} completed!')\n",
    "        print(f'   RMSE: ${rmse:.2f} | MAE: ${mae:.2f} | R²: {r2:.3f}')\n",
    "    \n",
    "    # Results comparison\n",
    "    clv_results_df = pd.DataFrame({k: {metric: v[metric] for metric in ['RMSE', 'MAE', 'R2_Score']} \n",
    "                                  for k, v in clv_results.items()}).T\n",
    "    \n",
    "    print('\n🏆 CLV MODEL PERFORMANCE COMPARISON')\n",
    "    print('=' * 60)\n",
    "    print(clv_results_df.round(3))\n",
    "    \n",
    "    # Best model selection\n",
    "    best_clv_model_name = clv_results_df['R2_Score'].idxmax()\n",
    "    best_clv_model = clv_results[best_clv_model_name]['Model']\n",
    "    \n",
    "    print(f'\n🥇 Best CLV model: {best_clv_model_name} (R²: {clv_results_df.loc[best_clv_model_name, "R2_Score"]:.3f})')\n",
    "    \n",
    "    # CLV segmentation\n",
    "    if best_clv_model_name == 'Linear Regression':\n",
    "        clv_predictions = best_clv_model.predict(scaler_clv.transform(X_clv))\n",
    "    else:\n",
    "        clv_predictions = best_clv_model.predict(X_clv)\n",
    "    \n",
    "    # Create CLV segments\n",
    "    clv_percentiles = np.percentile(clv_predictions, [25, 50, 75])\n",
    "    \n",
    "    def categorize_clv(clv_value):\n",
    "        if clv_value < clv_percentiles[0]:\n",
    "            return 'Low Value'\n",
    "        elif clv_value < clv_percentiles[1]:\n",
    "            return 'Medium-Low Value'\n",
    "        elif clv_value < clv_percentiles[2]:\n",
    "            return 'Medium-High Value'\n",
    "        else:\n",
    "            return 'High Value'\n",
    "    \n",
    "    df_processed['clv_predicted'] = clv_predictions\n",
    "    df_processed['clv_segment'] = df_processed['clv_predicted'].apply(categorize_clv)\n",
    "    \n",
    "    # CLV segment analysis\n",
    "    print('\n💎 CUSTOMER LIFETIME VALUE SEGMENTS')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    clv_segment_analysis = df_processed.groupby('clv_segment').agg({\n",
    "        'customer_id': 'count',\n",
    "        'clv_predicted': 'mean',\n",
    "        'monthly_charge': 'mean',\n",
    "        'tenure_months': 'mean',\n",
    "        'churn': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    clv_segment_analysis.columns = ['Customer_Count', 'Avg_Predicted_CLV', 'Avg_Monthly_Charge', \n",
    "                                   'Avg_Tenure_Months', 'Churn_Rate']\n",
    "    \n",
    "    print(clv_segment_analysis)\n",
    "    \n",
    "    return best_clv_model, clv_results_df, scaler_clv, clv_segment_analysis\n",
    "\n",
    "# Build CLV prediction model\n",
    "clv_model, clv_performance, clv_scaler, clv_segments = build_clv_prediction_model(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Deployment Functions\n",
    "\n",
    "Functions to use the trained models for predictions and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_churn_for_customer(customer_data, model, scaler, feature_names):\n",
    "    \"\"\"\n",
    "    Predict churn probability for a new customer\n",
    "    \"\"\"\n",
    "    # Ensure customer data has all required features\n",
    "    customer_df = pd.DataFrame([customer_data])\n",
    "    customer_processed = preprocess_data(customer_df)\n",
    "    \n",
    "    # Ensure all features are present\n",
    "    for feature in feature_names:\n",
    "        if feature not in customer_processed.columns:\n",
    "            customer_processed[feature] = 0\n",
    "    \n",
    "    # Select and order features\n",
    "    X_customer = customer_processed[feature_names]\n",
    "    \n",
    "    # Scale if using logistic regression\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        if str(type(model)).find('Logistic') != -1:\n",
    "            X_customer_scaled = scaler.transform(X_customer)\n",
    "            churn_probability = model.predict_proba(X_customer_scaled)[0][1]\n",
    "        else:\n",
    "            churn_probability = model.predict_proba(X_customer)[0][1]\n",
    "    else:\n",
    "        churn_probability = model.predict(X_customer)[0]\n",
    "    \n",
    "    return churn_probability\n",
    "\n",
    "def recommend_plan_for_customer(age, family_size, usage_gb, income_bracket, location):\n",
    "    \"\"\"\n",
    "    Recommend optimal subscription plan for a customer\n",
    "    \"\"\"\n",
    "    # Encode income bracket\n",
    "    income_mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "    income_encoded = income_mapping.get(income_bracket, 1)\n",
    "    \n",
    "    # Recommendation logic\n",
    "    if usage_gb > 200 and income_encoded == 2:  # High usage, high income\n",
    "        return 'Fibernet_Premium'\n",
    "    elif usage_gb > 200 and income_encoded == 1:  # High usage, medium income\n",
    "        return 'Fibernet_Basic'\n",
    "    elif family_size > 4:  # Large family\n",
    "        return 'Fibernet_Premium'\n",
    "    elif usage_gb < 50 and income_encoded == 0:  # Low usage, low income\n",
    "        return 'Broadband_Copper_Basic'\n",
    "    elif 50 <= usage_gb <= 200:  # Medium usage\n",
    "        return 'Broadband_Copper_Premium'\n",
    "    else:\n",
    "        return 'Fibernet_Basic'\n",
    "\n",
    "def calculate_optimal_price(plan_type, location, current_churn_rate):\n",
    "    \"\"\"\n",
    "    Calculate optimal pricing based on market analysis\n",
    "    \"\"\"\n",
    "    base_prices = {\n",
    "        'Fibernet_Premium': 120,\n",
    "        'Fibernet_Basic': 80,\n",
    "        'Broadband_Copper_Premium': 60,\n",
    "        'Broadband_Copper_Basic': 40\n",
    "    }\n",
    "    \n",
    "    base_price = base_prices.get(plan_type, 60)\n",
    "    \n",
    "    # Location adjustment\n",
    "    location_multiplier = {'Urban': 1.1, 'Suburban': 1.0, 'Rural': 0.9}\n",
    "    base_price *= location_multiplier.get(location, 1.0)\n",
    "    \n",
    "    # Churn rate adjustment\n",
    "    if current_churn_rate > 0.25:  # High churn\n",
    "        base_price *= 0.9  # Reduce price\n",
    "    elif current_churn_rate < 0.15:  # Low churn\n",
    "        base_price *= 1.05  # Slight increase\n",
    "    \n",
    "    return round(base_price, 2)\n",
    "\n",
    "# Example usage demonstrations\n",
    "print('🚀 MODEL DEPLOYMENT FUNCTIONS READY!')\n",
    "print('=' * 60)\n",
    "\n",
    "# Example 1: Churn prediction\n",
    "example_customer = {\n",
    "    'age': 45, 'gender': 'Male', 'location': 'Urban', 'income_bracket': 'High',\n",
    "    'family_size': 3, 'subscription_type': 'Fibernet_Premium', 'contract_type': 'Monthly',\n",
    "    'monthly_charge': 120, 'data_quota_gb': 500, 'tenure_months': 12,\n",
    "    'avg_monthly_usage_gb': 400, 'peak_usage_hours': 'Evening',\n",
    "    'support_tickets_3m': 2, 'payment_delays_6m': 0, 'auto_renew': 1\n",
    "}\n",
    "\n",
    "try:\n",
    "    churn_prob = predict_churn_for_customer(example_customer, churn_model, churn_scaler, feature_names)\n",
    "    print(f'\n🎯 Example Churn Prediction: {churn_prob:.3f} ({churn_prob:.1%} probability)')\n",
    "except Exception as e:\n",
    "    print(f'Churn prediction error: {e}')\n",
    "\n",
    "# Example 2: Plan recommendation\n",
    "recommended_plan = recommend_plan_for_customer(35, 4, 180, 'Medium', 'Suburban')\n",
    "print(f'🎯 Example Plan Recommendation: {recommended_plan}')\n",
    "\n",
    "# Example 3: Optimal pricing\n",
    "optimal_price = calculate_optimal_price('Fibernet_Basic', 'Urban', 0.18)\n",
    "print(f'💰 Example Optimal Price: ${optimal_price}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Performance Summary & Business Insights\n",
    "\n",
    "Comprehensive summary of all models and actionable business recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_business_insights(df, churn_results, segments_info, pricing_analysis):\n",
    "    \"\"\"\n",
    "    Generate comprehensive business insights from all models\n",
    "    \"\"\"\n",
    "    print('🏢 COMPREHENSIVE BUSINESS INSIGHTS & RECOMMENDATIONS')\n",
    "    print('=' * 80)\n",
    "    \n",
    "    # Overall metrics\n",
    "    total_customers = len(df)\n",
    "    churn_rate = df['churn'].mean()\n",
    "    total_monthly_revenue = df['monthly_charge'].sum()\n",
    "    avg_clv = df['clv'].mean()\n",
    "    \n",
    "    print('📊 KEY BUSINESS METRICS')\n",
    "    print('-' * 40)\n",
    "    print(f'Total Customers: {total_customers:,}')\n",
    "    print(f'Monthly Churn Rate: {churn_rate:.2%}')\n",
    "    print(f'Total Monthly Revenue: ${total_monthly_revenue:,.2f}')\n",
    "    print(f'Average Customer Lifetime Value: ${avg_clv:,.2f}')\n",
    "    print(f'Customer Acquisition Cost (Est.): ${avg_clv * 0.2:.2f}')\n",
    "    \n",
    "    # Churn insights\n",
    "    print('\n❌ CHURN ANALYSIS INSIGHTS')\n",
    "    print('-' * 40)\n",
    "    high_risk_customers = (df['monthly_charge'] > 100).sum()\n",
    "    monthly_contract_churn = df[df['contract_type'] == 'Monthly']['churn'].mean()\n",
    "    \n",
    "    print(f'High-price customers (>${100}): {high_risk_customers:,} ({high_risk_customers/total_customers:.1%})')\n",
    "    print(f'Monthly contract churn rate: {monthly_contract_churn:.2%}')\n",
    "    print(f'Potential revenue at risk: ${df[df["churn"] == 1]["monthly_charge"].sum() * 12:,.2f} annually')\n",
    "    \n",
    "    # Segment insights\n",
    "    print('\n👥 CUSTOMER SEGMENTATION INSIGHTS')\n",
    "    print('-' * 40)\n",
    "    print(segments_info)\n",
    "    \n",
    "    # Revenue optimization\n",
    "    print('\n💰 REVENUE OPTIMIZATION OPPORTUNITIES')\n",
    "    print('-' * 40)\n",
    "    print(f'Current ARPU: ${df["monthly_charge"].mean():.2f}')\n",
    "    \n",
    "    # Pricing insights\n",
    "    low_churn_revenue = 0\n",
    "    for plan_type in pricing_analysis.index:\n",
    "        if pricing_analysis.loc[plan_type, 'Churn_Rate'] < 0.15:\n",
    "            plan_customers = df[df['subscription_type'] == plan_type]\n",
    "            low_churn_revenue += plan_customers['monthly_charge'].sum()\n",
    "    \n",
    "    print(f'Revenue from low-churn plans: ${low_churn_revenue:,.2f}/month')\n",
    "    print(f'Potential 5% price increase impact: ${low_churn_revenue * 0.05:,.2f}/month')\n",
    "    \n",
    "    # Strategic recommendations\n",
    "    print('\n🎯 STRATEGIC RECOMMENDATIONS')\n",
    "    print('-' * 40)\n",
    "    \n",
    "    recommendations = [\n",
    "        '1. REDUCE CHURN:',\n",
    "        '   • Implement proactive retention for customers with >3 support tickets',\n",
    "        '   • Offer yearly contract discounts to reduce monthly contract churn',\n",
    "        '   • Create loyalty programs for customers with >24 months tenure',\n",
    "        '',\n",
    "        '2. OPTIMIZE PRICING:',\n",
    "        '   • Increase prices by 5% on low-churn, popular plans',\n",
    "        '   • Reduce prices by 10% on high-churn premium plans',\n",
    "        '   • Introduce usage-based pricing for heavy users',\n",
    "        '',\n",
    "        '3. IMPROVE CUSTOMER EXPERIENCE:',\n",
    "        '   • Focus support resources during evening peak hours',\n",
    "        '   • Implement auto-upgrade recommendations for quota-exceeding users',\n",
    "        '   • Create family plans for households with >4 members',\n",
    "        '',\n",
    "        '4. REVENUE GROWTH:',\n",
    "        '   • Target high-value customer segments in urban areas',\n",
    "        '   • Develop premium services for heavy data users',\n",
    "        '   • Implement referral programs in suburban markets'\n",
    "    ]\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        print(rec)\n",
    "    \n",
    "    # ROI calculations\n",
    "    print('\n💼 EXPECTED ROI FROM IMPLEMENTATIONS')\n",
    "    print('-' * 40)\n",
    "    \n",
    "    # Churn reduction ROI\n",
    "    current_churn_cost = df[df['churn'] == 1]['clv'].sum()\n",
    "    potential_churn_reduction = current_churn_cost * 0.3  # 30% reduction\n",
    "    \n",
    "    # Pricing optimization ROI\n",
    "    pricing_roi = low_churn_revenue * 0.05 * 12  # 5% annual increase\n",
    "    \n",
    "    print(f'Churn reduction (30% improvement): ${potential_churn_reduction:,.2f} value retention')\n",
    "    print(f'Pricing optimization: ${pricing_roi:,.2f} additional annual revenue')\n",
    "    print(f'Total potential annual impact: ${potential_churn_reduction + pricing_roi:,.2f}')\n",
    "    \n",
    "    return {\n",
    "        'total_customers': total_customers,\n",
    "        'churn_rate': churn_rate,\n",
    "        'monthly_revenue': total_monthly_revenue,\n",
    "        'avg_clv': avg_clv,\n",
    "        'potential_annual_impact': potential_churn_reduction + pricing_roi\n",
    "    }\n",
    "\n",
    "# Generate comprehensive insights\n",
    "business_metrics = generate_business_insights(df, churn_results, segments_info, pricing_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Models and Export Results\n",
    "\n",
    "Save trained models and export analysis results for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('subscription_management_models', exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "model_artifacts = {\n",
    "    'churn_model': churn_model,\n",
    "    'churn_scaler': churn_scaler,\n",
    "    'feature_names': feature_names,\n",
    "    'recommendation_kmeans': rec_kmeans,\n",
    "    'recommendation_scaler': rec_scaler,\n",
    "    'clv_model': clv_model,\n",
    "    'clv_scaler': clv_scaler\n",
    "}\n",
    "\n",
    "for name, artifact in model_artifacts.items():\n",
    "    with open(f'subscription_management_models/{name}.pkl', 'wb') as f:\n",
    "        pickle.dump(artifact, f)\n",
    "\n",
    "# Export analysis results to CSV\n",
    "results_to_export = {\n",
    "    'churn_model_performance': churn_results,\n",
    "    'customer_segments': segments_info,\n",
    "    'pricing_analysis': pricing_analysis,\n",
    "    'clv_model_performance': clv_performance,\n",
    "    'clv_segments': clv_segments\n",
    "}\n",
    "\n",
    "for name, result_df in results_to_export.items():\n",
    "    if isinstance(result_df, pd.DataFrame):\n",
    "        result_df.to_csv(f'subscription_management_models/{name}.csv')\n",
    "\n",
    "# Export customer data with predictions\n",
    "df_with_predictions = df.copy()\n",
    "df_with_predictions['customer_segment'] = df_with_recs['customer_segment']\n",
    "df_with_predictions['recommended_plan'] = df_with_recs['recommended_plan']\n",
    "df_with_predictions.to_csv('subscription_management_models/customer_data_with_predictions.csv', index=False)\n",
    "\n",
    "print('✅ MODELS AND RESULTS SAVED SUCCESSFULLY!')\n",
    "print('=' * 60)\n",
    "print('Saved files:')\n",
    "for filename in os.listdir('subscription_management_models'):\n",
    "    print(f'  📄 {filename}')\n",
    "\n",
    "print('\n🎯 SUBSCRIPTION MANAGEMENT ML PIPELINE COMPLETE!')\n",
    "print('All models trained, evaluated, and ready for deployment.')\n",
    "\n",
    "# Final summary\n",
    "print('\n📋 FINAL MODEL SUMMARY:')\n",
    "print('-' * 30)\n",
    "print(f'🤖 Churn Prediction: {churn_results["AUC-ROC"].max():.3f} AUC-ROC')\n",
    "print(f'🎯 Plan Recommendations: {len(segments_info)} customer segments identified')\n",
    "print(f'💰 Pricing Strategy: {len(pricing_recommendations)} optimization recommendations')\n",
    "print(f'💎 CLV Prediction: {clv_performance["R2_Score"].max():.3f} R² Score')\n",
    "print(f'📊 Business Impact: ${business_metrics["potential_annual_impact"]:,.0f} annual potential')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
